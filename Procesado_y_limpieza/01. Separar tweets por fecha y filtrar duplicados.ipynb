{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We import all necessary libraries.\n",
    "from pathlib import Path\n",
    "import ujson\n",
    "import glob\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that adds a unique id to its list by date.\n",
    "def unique_found(unique_id): \n",
    "    unique_id.add(tweet_id)\n",
    "    \n",
    "# Function that writes a tweet in a file.\n",
    "def write_to_file(file_by_date):\n",
    "    with open(file_by_date, \"a\") as outfile: \n",
    "        outfile.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists of ids by date.\n",
    "ids_pre_6M = []\n",
    "ids_6M = []\n",
    "ids_7M = []\n",
    "ids_8M = []\n",
    "ids_9M = []\n",
    "ids_10M = []\n",
    "ids_post_10M = []\n",
    "\n",
    "# List of unique ids by date.\n",
    "unique_ids_pre_6M = set(ids_pre_6M)\n",
    "unique_ids_6M = set(ids_6M)\n",
    "unique_ids_7M = set(ids_7M)\n",
    "unique_ids_8M = set(ids_8M)\n",
    "unique_ids_9M = set(ids_9M)\n",
    "unique_ids_10M = set(ids_10M)\n",
    "unique_ids_post_10M = set(ids_post_10M)\n",
    "\n",
    "# Date limits to use for date classification.\n",
    "border_5M_6M = datetime.datetime.strptime(\"05/03/2020 22:59:59\", \"%d/%m/%Y %H:%M:%S\")\n",
    "border_6M_7M = datetime.datetime.strptime(\"06/03/2020 22:59:59\", \"%d/%m/%Y %H:%M:%S\")\n",
    "border_7M_8M = datetime.datetime.strptime(\"07/03/2020 22:59:59\", \"%d/%m/%Y %H:%M:%S\")\n",
    "border_8M_9M = datetime.datetime.strptime(\"08/03/2020 22:59:59\", \"%d/%m/%Y %H:%M:%S\")\n",
    "border_9M_10M = datetime.datetime.strptime(\"09/03/2020 22:59:59\", \"%d/%m/%Y %H:%M:%S\")\n",
    "border_10M_11M = datetime.datetime.strptime(\"10/03/2020 22:59:59\", \"%d/%m/%Y %H:%M:%S\")\n",
    "\n",
    "# Output files by date.\n",
    "file_6M = r'C:\\UOC\\TFM\\Procesado_tweets\\01_Clasificacion_por_fecha\\6M.json'\n",
    "file_7M = r'C:\\UOC\\TFM\\Procesado_tweets\\01_Clasificacion_por_fecha\\7M.json'\n",
    "file_8M = r'C:\\UOC\\TFM\\Procesado_tweets\\01_Clasificacion_por_fecha\\8M.json'\n",
    "file_9M = r'C:\\UOC\\TFM\\Procesado_tweets\\01_Clasificacion_por_fecha\\9M.json'\n",
    "file_10M = r'C:\\UOC\\TFM\\Procesado_tweets\\01_Clasificacion_por_fecha\\10M.json'\n",
    "\n",
    "# Counters by date to know how many tweets were published. \n",
    "count_pre_6M = 0\n",
    "count_6M = 0\n",
    "count_7M = 0\n",
    "count_8M = 0\n",
    "count_9M = 0\n",
    "count_10M = 0\n",
    "count_post_10M = 0\n",
    "\n",
    "# Input folder.\n",
    "folder = r'C:\\UOC\\TFM\\Procesado_tweets\\00_Archivos_originales'\n",
    "\n",
    "# Iterate over all files located in the defined folder.\n",
    "for file in Path(folder).glob('*.json'):\n",
    "    # For each line of the file.\n",
    "    for line in open(file, 'r', encoding=\"utf8\"):\n",
    "        # Skip empty lines.\n",
    "        if not line.strip (): \n",
    "            continue\n",
    "        # If the line contains a tweet.                   \n",
    "        if line.startswith('{\"created_at\":') or line.startswith('{\"contributors\":'):\n",
    "            # Get the publication date of the tweet, and convert it to a datetime object.     \n",
    "            tweet_orig_date = ujson.loads(line)[\"created_at\"]\n",
    "            processed_date = datetime.datetime.strptime(tweet_orig_date,'%a %b %d %H:%M:%S +0000 %Y')\n",
    "            # Get the tweet's id.    \n",
    "            tweet_id = ujson.loads(line)['id']\n",
    "        \n",
    "\n",
    "            if processed_date <= border_5M_6M:\n",
    "                # If it is not a duplicate, add its id into the corresponding unique ids list, and update its counter.                \n",
    "                if tweet_id not in unique_ids_pre_6M:\n",
    "                    unique_found(unique_ids_pre_6M)\n",
    "                    count_pre_6M += 1\n",
    "                    \n",
    "            elif (processed_date > border_5M_6M and processed_date <= border_6M_7M):\n",
    "                if tweet_id not in unique_ids_6M:\n",
    "                    unique_found(unique_ids_6M)\n",
    "                    write_to_file(file_6M)\n",
    "                    count_6M += 1\n",
    "                    \n",
    "            elif (processed_date > border_6M_7M and processed_date <= border_7M_8M):\n",
    "                if tweet_id not in unique_ids_7M:\n",
    "                    unique_found(unique_ids_7M)\n",
    "                    write_to_file(file_7M)\n",
    "                    count_7M += 1\n",
    "                    \n",
    "            elif (processed_date > border_7M_8M and processed_date <= border_8M_9M):\n",
    "                if tweet_id not in unique_ids_8M:\n",
    "                    unique_found(unique_ids_8M)\n",
    "                    write_to_file(file_8M) \n",
    "                    count_8M += 1\n",
    "                    \n",
    "            elif (processed_date > border_8M_9M and processed_date <= border_9M_10M):\n",
    "                if tweet_id not in unique_ids_9M:\n",
    "                    unique_found(unique_ids_9M)\n",
    "                    write_to_file(file_9M)\n",
    "                    count_9M += 1\n",
    "                    \n",
    "            elif (processed_date > border_9M_10M and processed_date <= border_10M_11M):\n",
    "                if tweet_id not in unique_ids_10M:\n",
    "                    unique_found(unique_ids_10M)\n",
    "                    write_to_file(file_10M)\n",
    "                    count_10M += 1\n",
    "                    \n",
    "            else:\n",
    "                if tweet_id not in unique_ids_post_10M:\n",
    "                    unique_found(unique_ids_post_10M)\n",
    "                    count_post_10M += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de tweets anteriores al 6M: 126943\n",
      "Número de tweets el 6M: 80476\n",
      "Número de tweets el 7M: 95294\n",
      "Número de tweets el 8M: 1057057\n",
      "Número de tweets el 9M: 413642\n",
      "Número de tweets el 10M: 91931\n",
      "Número de tweets después del 10M: 20931\n"
     ]
    }
   ],
   "source": [
    "# Print the number of tweets by date.\n",
    "print(\"Número de tweets anteriores al 6M: {}\".format(count_pre_6M))\n",
    "print(\"Número de tweets el 6M: {}\".format(count_6M))\n",
    "print(\"Número de tweets el 7M: {}\".format(count_7M))\n",
    "print(\"Número de tweets el 8M: {}\".format(count_8M))\n",
    "print(\"Número de tweets el 9M: {}\".format(count_9M))\n",
    "print(\"Número de tweets el 10M: {}\".format(count_10M))\n",
    "print(\"Número de tweets después del 10M: {}\".format(count_post_10M))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
